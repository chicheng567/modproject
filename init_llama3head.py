import torch
from independent_llama32 import LlamaHead
from transformers import AutoTokenizer
import json
from transformers.models.llama.configuration_llama import LlamaConfig

def get_llama3_head(config: LlamaConfig):
    # è¼‰å…¥ llama3.2 çš„æ¬Šé‡æª”æ¡ˆ
    state_dict = torch.load("llama3_model_weights_bf16.pth", map_location="cpu")
    model_head = LlamaHead(config=config)
    head_state_dict = {}

    # è¼‰å…¥ embedding æ¬Šé‡
    if "model.embed_tokens.weight" in state_dict:
        head_state_dict["embed_tokens.weight"] = state_dict["model.embed_tokens.weight"]
        print("âœ“ Loaded embed_tokens.weight")
    elif "embed_tokens.weight" in state_dict:
        head_state_dict["embed_tokens.weight"] = state_dict["embed_tokens.weight"]
        print("âœ“ Loaded embed_tokens.weight (alternative path)")

    # å®šç¾©ç¬¬ä¸€å±¤çš„å‰ç¶´
    layer_0_prefix = "model.layers.0."
    alt_layer_0_prefix = "layers.0."

    # è¼‰å…¥ç¬¬ä¸€å±¤çš„æ¬Šé‡
    for key in state_dict.keys():
        if key.startswith(layer_0_prefix):
            new_key = "first_layer." + key[len(layer_0_prefix):]
            head_state_dict[new_key] = state_dict[key]
            print(f"âœ“ Loaded {new_key}")
        elif key.startswith(alt_layer_0_prefix):
            new_key = "first_layer." + key[len(alt_layer_0_prefix):]
            head_state_dict[new_key] = state_dict[key]
            print(f"âœ“ Loaded {new_key}")

    # è¼‰å…¥ norm æ¬Šé‡
    if "model.norm.weight" in state_dict:
        head_state_dict["norm.weight"] = state_dict["model.norm.weight"]
        print("âœ“ Loaded norm.weight")
    elif "norm.weight" in state_dict:
        head_state_dict["norm.weight"] = state_dict["norm.weight"]
        print("âœ“ Loaded norm.weight (alternative path)")

    print(f"\nå‰µå»ºäº†åŒ…å« {len(head_state_dict)} å€‹åƒæ•¸çš„ head_state_dict")

    print("\nLlamaHead é æœŸçš„åƒæ•¸ï¼š")
    for name, param in model_head.named_parameters():
        print(f"  {name}: {param.shape}")

    print("\nhead_state_dict ä¸­çš„åƒæ•¸ï¼š")
    for name, tensor in head_state_dict.items():
        print(f"  {name}: {tensor.shape}")

    model_params = set(name for name, _ in model_head.named_parameters())
    loaded_params = set(head_state_dict.keys())

    missing_params = model_params - loaded_params
    extra_params = loaded_params - model_params

    if missing_params:
        print(f"\nâš ï¸  ç¼ºå°‘çš„åƒæ•¸: {missing_params}")
    if extra_params:
        print(f"\nâš ï¸  é¡å¤–çš„åƒæ•¸: {extra_params}")

    try:
        missing_keys, unexpected_keys = model_head.load_state_dict(head_state_dict, strict=False)
        
        if missing_keys:
            print(f"\nğŸ“ ç¼ºå°‘çš„éµ (å°‡ä½¿ç”¨é è¨­åˆå§‹åŒ–): {missing_keys}")
        if unexpected_keys:
            print(f"\nğŸ“ æ„å¤–çš„éµ: {unexpected_keys}")
            
        print("\nâœ… æˆåŠŸè¼‰å…¥ state_dict åˆ° LlamaHeadï¼")
        
    except Exception as e:
        print(f"\nâŒ è¼‰å…¥ state_dict æ™‚å‡ºéŒ¯: {e}")
        
        print("\nå˜—è©¦æ‰‹å‹•åƒæ•¸åŒ¹é…...")
        for name, param in model_head.named_parameters():
            if name in head_state_dict:
                if param.shape == head_state_dict[name].shape:
                    param.data.copy_(head_state_dict[name])
                    print(f"âœ“ æ‰‹å‹•è¼‰å…¥ {name}")
                else:
                    print(f"âŒ {name} çš„å½¢ç‹€ä¸åŒ¹é…: é æœŸ {param.shape}, å¯¦éš› {head_state_dict[name].shape}")
            else:
                print(f"âš ï¸  åœ¨ state_dict ä¸­æ‰¾ä¸åˆ°åƒæ•¸ {name}")

    # å°‡æ¨¡å‹ç§»è‡³ GPU ä¸¦è¨­å®šç‚º bfloat16
    model_head.to("cuda:0", dtype=torch.bfloat16)

    print("\næª¢æŸ¥åƒæ•¸è³‡æ–™é¡å‹ï¼š")
    all_bfloat16 = True
    for name, param in model_head.named_parameters():
        if param.dtype != torch.bfloat16:
            print(f"âŒ åƒæ•¸ {name} çš„è³‡æ–™é¡å‹ç‚º {param.dtype} è€Œé bfloat16")
            all_bfloat16 = False

    if all_bfloat16:
        print("âœ… æ‰€æœ‰åƒæ•¸éƒ½æ˜¯ bfloat16 æ ¼å¼")
    return model_head

def main():
    # è¼‰å…¥ llama3.2 é…ç½®
    with open("llama3_config.json", "r") as f:
        config_dict = json.load(f)
    
    config = LlamaConfig(**config_dict)
    
    print("é–‹å§‹è¼‰å…¥ Llama3.2 Head...")
    model_head = get_llama3_head(config)
    
    print("\nğŸš€ Llama3.2 Head è¼‰å…¥å®Œæˆï¼")
    return model_head

if __name__ == "__main__":
    main()
